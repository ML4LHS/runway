---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)

knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# runway

<!-- badges: start -->
[![Lifecycle: maturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)
<!-- badges: end -->

The goal of runway is to generate statistics and plots to calculate discrimination, calibration, and decision curves for prediction models.

## Why is it called runway?

Because you can use it to visually compare models.

Sometimes your models look quite different.

![Models looking different](https://i.pinimg.com/originals/2e/3d/14/2e3d14e6f382c6850685b5aaaff34fec.gif)

Other times your models look the same...

![Models looking similar](https://pbs.twimg.com/media/Eg7RZKoXcAAhvKw?format=jpg&name=360x360)

## Installation

You can install `runway` from GitHub with:

```{r eval=FALSE}
remotes::install_github('ML4LHS/runway')
```

## Load the package

First, load the package.

```{r}
library(runway)
```

## Sample datasets

Runway comes with two sample datasets.

```{r}
data(single_model_dataset)
head(single_model_dataset)

data(multi_model_dataset)
head(multi_model_dataset)
```


## Evaluating a single model

### Threshold-performance plot (single model)

```{r}
threshperf_plot(single_model_dataset,
                outcome = 'outcomes',
                prediction = 'predictions')
```


### Calibration plot with 10 bins (single model)

Note: 10 bins is the default.

```{r}
cal_plot(single_model_dataset,
         outcome = 'outcomes', 
         prediction = 'predictions')
```

### Calibration plot with 5 bins (single model)

```{r}
cal_plot(single_model_dataset,
         outcome = 'outcomes', 
         prediction = 'predictions',
         n_bins = 5)
```


### Calibration plot with 10 bins and loess curve (single model)

```{r}
cal_plot(single_model_dataset,
         outcome = 'outcomes', 
         prediction = 'predictions',
         show_loess = TRUE)
```

### Calibration plot with loess curve only (single model)

```{r}
cal_plot(single_model_dataset,
         outcome = 'outcomes', 
         prediction = 'predictions',
         n_bins = 0,
         show_loess = TRUE)
```


## Comparing multiple models


### Threshold-performance plot (multiple models)

```{r}
threshperf_plot_multi(multi_model_dataset,
                      outcome = 'outcomes',
                      prediction = 'predictions',
                      model = 'model_name')
```

### Calibration plot with 10 bins (multiple models)

Note: 10 bins is the default.

```{r}
cal_plot_multi(multi_model_dataset,
         outcome = 'outcomes',
         prediction = 'predictions',
         model = 'model_name')
```

## Calibration plot with 5 bins (multiple models)

```{r}
cal_plot_multi(multi_model_dataset,
         outcome = 'outcomes',
         prediction = 'predictions',
         model = 'model_name',
         n_bins = 5)
```


## Calibration plot with loess curve (multiple models)

Unlike single calibration plots, the choice of binned calibration and loess calibration are mutually exclusive. To show less curves, you must set `show_loess` to `TRUE` *and* `n_bins` to `0`.

```{r}
cal_plot_multi(multi_model_dataset,
         outcome = 'outcomes',
         prediction = 'predictions',
         model = 'model_name',
         n_bins = 0,
         show_loess = TRUE)
```

## ROC curve w/CI

```{r}
roc_plot(single_model_dataset, 
         outcome = 'outcomes', 
         prediction = 'predictions',
         ci = TRUE, 
         plot_title = 'Single ROC curve w/CI ribbon')
```

## Multiple ROC curves w/CI ribbons

```{r}
roc_plot_multi(multi_model_dataset, 
         outcome = 'outcomes', 
         prediction = 'predictions', 
         model = 'model_name',
         ci = TRUE,
         plot_title = 'Multiple model ROC curves w/CI ribbons')
```
